# Tokenizer
Python based tokenizer made to identify lexical tokens as part of learning about programming languages. Currently only written for compatability with .jack files.

To be updated small error regarding detection of spaces inside text strings. 
